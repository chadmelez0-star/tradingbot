# ==========================================
# YAPAY ZEKA MOTORU - ELMAS BOT (TAM DÃœZELTME)
# ==========================================

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import os
from datetime import datetime

class AITradingEngine:
    def __init__(self):
        self.model_path = 'ai_models/trained_model.pkl'
        self.scaler_path = 'ai_models/scaler.pkl'
        self.model = None
        self.scaler = None
        self.feature_importance = {}
        self.is_trained = False
        self.feature_columns = None
        
    def create_advanced_features(self, df, for_training=True):
        """GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi - DÃœZELTÄ°LMÄ°Å"""
        if df is None or len(df) < 50:
            print(f"  âš ï¸ Yetersiz ham veri: {len(df) if df is not None else 0}")
            return pd.DataFrame()
        
        print(f"  ğŸ“Š Ham veri: {len(df)} satÄ±r")
        
        # Kopya al ve tip dÃ¶nÃ¼ÅŸÃ¼mleri yap
        data = df.copy()
        
        # Temel kolonlarÄ± kontrol et
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in required_cols:
            if col not in data.columns:
                print(f"  âŒ Eksik kolon: {col}")
                return pd.DataFrame()
            data[col] = pd.to_numeric(data[col], errors='coerce')
        
        features = pd.DataFrame(index=data.index)
        
        # 1. TEMEL FÄ°YAT Ã–ZELLÄ°KLERÄ° (en az hesaplama gerektiren)
        features['close'] = data['close']
        features['returns'] = data['close'].pct_change()
        features['log_returns'] = np.log(data['close'] / data['close'].shift(1))
        
        # 2. VOLATÄ°LÄ°TE (kÄ±sa window ile baÅŸla)
        features['volatility_5'] = features['returns'].rolling(window=5, min_periods=1).std()
        features['volatility_10'] = features['returns'].rolling(window=10, min_periods=1).std()
        
        # 3. HAREKETLÄ° ORTALAMALAR (min_periods=1 ile)
        for period in [5, 10, 20]:
            sma = data['close'].rolling(window=period, min_periods=1).mean()
            features[f'sma_{period}'] = sma
            features[f'ema_{period}'] = data['close'].ewm(span=period, adjust=False, min_periods=1).mean()
            features[f'distance_sma_{period}'] = (data['close'] - sma) / sma
        
        # 4. BASÄ°T FÄ°YAT Ã–ZELLÄ°KLERÄ°
        features['high_low_pct'] = (data['high'] - data['low']) / data['close']
        features['open_close_pct'] = (data['close'] - data['open']) / data['open']
        
        # 5. HACÄ°M Ã–ZELLÄ°KLERÄ°
        features['volume'] = data['volume']
        features['volume_sma_5'] = data['volume'].rolling(window=5, min_periods=1).mean()
        features['volume_ratio'] = data['volume'] / features['volume_sma_5']
        
        # 6. BASÄ°T RSI (14 period ama min_periods=5)
        delta = data['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(window=14, min_periods=5).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=5).mean()
        rs = gain / loss.replace(0, np.nan)
        features['rsi_14'] = 100 - (100 / (1 + rs))
        
        # 7. BASÄ°T MACD
        ema_12 = data['close'].ewm(span=12, adjust=False, min_periods=1).mean()
        ema_26 = data['close'].ewm(span=26, adjust=False, min_periods=1).mean()
        features['macd'] = ema_12 - ema_26
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False, min_periods=1).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']
        
        # 8. BOLLINGER BANDS (20 period, min_periods=5)
        bb_middle = data['close'].rolling(window=20, min_periods=5).mean()
        bb_std = data['close'].rolling(window=20, min_periods=5).std()
        features['bb_middle'] = bb_middle
        features['bb_upper'] = bb_middle + (bb_std * 2)
        features['bb_lower'] = bb_middle - (bb_std * 2)
        features['bb_width'] = (features['bb_upper'] - features['bb_lower']) / bb_middle
        features['bb_position'] = (data['close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])
        
        # 9. MOMENTUM
        features['momentum_5'] = data['close'] / data['close'].shift(5) - 1
        features['momentum_10'] = data['close'] / data['close'].shift(10) - 1
        
        # Hedef deÄŸiÅŸkenler (eÄŸitim modu iÃ§in)
        if for_training:
            # Gelecek 3 mumda %1 kazanÃ§? (daha esnek)
            future_return = data['close'].shift(-3) / data['close'] - 1
            features['target_direction'] = (future_return > 0.01).astype(int)
            features['target_return'] = future_return
        
        # NaN ve Inf temizliÄŸi - Dikkatli yap
        print(f"  ğŸ”§ Ã–zellikler oluÅŸturuldu: {len(features)} satÄ±r")
        
        # Ã–nce sadece tamamen NaN olan satÄ±rlarÄ± at
        features = features.dropna(how='all')
        
        # Sonra kalan NaN'larÄ± 0 ile doldur (Ã§ok az olmalÄ±)
        features = features.replace([np.inf, -np.inf], np.nan)
        features = features.fillna(0)
        
        print(f"  âœ… Temizlik sonrasÄ±: {len(features)} satÄ±r")
        
        return features
    
    def train(self, historical_data_dict):
        """Model eÄŸitimi - BÃœYÃœK VERÄ° Ä°Ã‡Ä°N OPTÄ°MÄ°ZE"""
        print("ğŸ§  AI Modeli eÄŸitiliyor (BÃ¼yÃ¼k veri seti)...")
        
        all_features = []
        all_targets = []
        total_samples = 0
        
        for symbol, df in historical_data_dict.items():
            try:
                print(f"\n  ğŸ“Š {symbol} iÅŸleniyor...")
                features = self.create_advanced_features(df, for_training=True)
                
                if len(features) < 50:  # â† Minimum 50 Ã¶rnek
                    print(f"  âš ï¸ {symbol}: Yetersiz Ã¶zellik verisi ({len(features)})")
                    continue
                
                if 'target_direction' not in features.columns:
                    print(f"  âš ï¸ {symbol}: Hedef deÄŸiÅŸken yok")
                    continue
                
                y = features['target_direction']
                X = features.drop(['target_direction', 'target_return'], axis=1, errors='ignore')
                X = X.select_dtypes(include=[np.number])
                
                if X.sum().sum() == 0:
                    print(f"  âš ï¸ {symbol}: TÃ¼m deÄŸerler 0")
                    continue
                
                all_features.append(X)
                all_targets.append(y)
                total_samples += len(X)
                print(f"  âœ… {symbol}: {len(X)} Ã¶rnek eklendi")
                    
            except Exception as e:
                print(f"  âŒ {symbol} atlandÄ±: {e}")
        
        if not all_features or total_samples < 500:  # â† Minimum 500 Ã¶rnek
            print(f"âŒ EÄŸitim iÃ§in yeterli veri yok (Toplam: {total_samples})")
            return False
        
        # BirleÅŸtir
        X = pd.concat(all_features, ignore_index=True)
        y = pd.concat(all_targets, ignore_index=True)
        
        print(f"\nğŸ“ˆ Toplam eÄŸitim verisi: {len(X):,} Ã¶rnek, {len(X.columns)} Ã¶zellik")
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
        class_counts = y.value_counts()
        print(f"ğŸ“Š SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±: {dict(class_counts)}")
        
        # Dengesizlik varsa uyarÄ±
        min_class = class_counts.min()
        max_class = class_counts.max()
        imbalance_ratio = min_class / max_class
        print(f"âš–ï¸ Dengesizlik oranÄ±: %{imbalance_ratio*100:.1f}")
        
        if imbalance_ratio < 0.3:
            print("âš ï¸ Veri seti Ã§ok dengesiz, sonuÃ§lar yanÄ±ltÄ±cÄ± olabilir")
        
        # EÄŸitim/test ayrÄ±mÄ± (stratify ile dengeli)
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
        except:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
        
        # Ã–lÃ§eklendirme
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Daha bÃ¼yÃ¼k veri iÃ§in optimize model
        n_estimators = min(200, max(50, len(X) // 100))  # â† Veri boyutuna gÃ¶re ayarla
        
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=12,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced_subsample'  # â† Dengesiz veri iÃ§in daha iyi
        )
        
        print(f"\nğŸ¤– Model eÄŸitiliyor ({n_estimators} aÄŸaÃ§)...")
        self.model.fit(X_train_scaled, y_train)
        
        # Performans
        train_score = self.model.score(X_train_scaled, y_train)
        test_score = self.model.score(X_test_scaled, y_test)
        
        # Overfitting kontrolÃ¼
        overfit_gap = train_score - test_score
        print(f"\nğŸ“Š Performans:")
        print(f"   EÄŸitim doÄŸruluÄŸu: %{train_score*100:.2f}")
        print(f"   Test doÄŸruluÄŸu: %{test_score*100:.2f}")
        print(f"   Fark: %{overfit_gap*100:.2f} ({'âš ï¸ Overfitting!' if overfit_gap > 0.15 else 'âœ… Normal'})")
        
        # Ã–zellik Ã¶nemleri
        importance = pd.DataFrame({
            'feature': X.columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        self.feature_importance = importance.head(10).to_dict('records')
        self.feature_columns = list(X.columns)
        
        # Kaydet
        os.makedirs('ai_models', exist_ok=True)
        joblib.dump(self.model, self.model_path)
        joblib.dump(self.scaler, self.scaler_path)
        joblib.dump(self.feature_columns, 'ai_models/feature_columns.pkl')
        
        self.is_trained = True
        
        print(f"\nğŸ“Š En Ã¶nemli 5 Ã¶zellik:")
        for i, row in importance.head(5).iterrows():
            print(f"   {row['feature']}: %{row['importance']*100:.2f}")
        
        return True    
    def predict(self, current_df):
        """Tahmin yap - DÃœZELTÄ°LMÄ°Å"""
        if not self.is_trained:
            if os.path.exists(self.model_path) and os.path.exists(self.scaler_path):
                try:
                    self.model = joblib.load(self.model_path)
                    self.scaler = joblib.load(self.scaler_path)
                    self.feature_columns = joblib.load('ai_models/feature_columns.pkl')
                    self.is_trained = True
                except Exception as e:
                    print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
                    return self._default_prediction()
            else:
                return self._default_prediction()
        
        try:
            features = self.create_advanced_features(current_df, for_training=False)
            if len(features) == 0:
                return self._default_prediction()
            
            # Son satÄ±rÄ± al
            X = features.select_dtypes(include=[np.number])
            
            # Kolon eÅŸleÅŸtirme
            if self.feature_columns:
                for col in self.feature_columns:
                    if col not in X.columns:
                        X[col] = 0
                X = X[self.feature_columns]
            
            X_last = X.iloc[-1:].values
            X_scaled = self.scaler.transform(X_last)
            
            probability = self.model.predict_proba(X_scaled)[0][1]
            prediction = self.model.predict(X_scaled)[0]
            
            confidence = probability if prediction == 1 else (1 - probability)
            
            if confidence > 0.75:
                signal = 'GÃœÃ‡LÃœ AL'
            elif confidence > 0.6:
                signal = 'AL'
            elif confidence < 0.25:
                signal = 'GÃœÃ‡LÃœ SAT'
            elif confidence < 0.4:
                signal = 'SAT'
            else:
                signal = 'BEKLE'
            
            return {
                'confidence': round(confidence * 100, 2),
                'signal': signal,
                'probability': round(probability, 4),
                'prediction': int(prediction)
            }
            
        except Exception as e:
            print(f"âŒ AI tahmin hatasÄ±: {e}")
            return self._default_prediction()
    
    def _default_prediction(self):
        """VarsayÄ±lan tahmin"""
        return {'confidence': 50.0, 'signal': 'BEKLE', 'probability': 0.5, 'prediction': 0}
    
    def get_feature_importance(self):
        return self.feature_importance